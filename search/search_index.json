{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the ASL Trunk robot documentation","text":"<p>This documentation provides a detailed guide to the setup, configuration, and use of the Trunk Robot. It is intended for internal use by the team involved in the development, deployment, and maintenance of the robot.</p>"},{"location":"#contents","title":"Contents","text":"<ul> <li>Design: Documentation for full-system software, electrical, and mechanical design. Inlcudes BOM, CAD assets, circuit diagram, and design considerations.</li> <li>Robot Setup: Instructions for setting up trunk hardware and software</li> <li>Usage: Instructions for using a set up trunk robot<ul> <li>Motion Capture: Instructions for setting up and using the motion capture system with the Trunk Robot.</li> <li>Motor Control: Instructions for enabling motor control of the Trunk Robot.</li> <li>Collecting Data: Instructions for collecting data using the Trunk Robot, including data collection scripts and procedures.</li> <li>Video Streaming: Instructions for using the video streaming capabilities of the Trunk Robot.</li> <li>Telemetry Viewer: Description on how to view and visualize telemetry data.</li> <li>Teleoperation: Instructions for teleoperating the robot with an Apple Vision Pro to collect data.</li> <li>Visuomotor Rollout: Instructions for rolling out a visuomotor policy on the Trunk Robot hardware.</li> </ul> </li> <li>Contributing: Guidelines for contributing to the development of the Trunk Robot project.</li> </ul> <p>This documentation is intended as a practical resource to support your work with the Trunk Robot, ensuring that all ASL members have access to the necessary information to effectively manage the system.</p>"},{"location":"3dprinting/","title":"3D Printing","text":"<p>Many of the components for the trunk robot are custom and 3d printed. All of the 3d printed components in the assembly can eaily be printed on any commercial or hobbyist FDM printer. We utilized a Bambu X1C with AMS to print all 3d printed components with PLA, but other materials could alternately be used.</p>"},{"location":"3dprinting/#printer-settings","title":"Printer Settings","text":"<p>On the Bambu X1C, we used default print settings (infill of 15% and 2 wall loops) on the Bambu Cool Plate, with PLA as the primary material, PLA as the support material, and support PLA as the support/raft interface material on all prints except those listed below: - Pulleys: Printed with PLA-CF, PLA-CF support material, infill of 100% and 5 wall loops - Trunk Disks: Infill of 30% and 5 wall loops</p> <p>We found that an offset of 7 thousandths of an inch (0.18 mm) for a friction fit between manufacturer parts and 3d printed parts. For example, if a manufacturer part is friction fit into a custom 3d printed part, and the outer diameter of a manufacturer part is 1.000\", then our 3d printed part would have an internal diameter of 1.007\". This tolerance may change on other 3d printers, but we found it to be consistent across prints on the Bambu X1C.</p>"},{"location":"3dprinting/#assets","title":"Assets","text":"<p>All 3d printed assets are available in the full CAD assembly. All assets are editable and fully configurable in Fusion360. Please email mleone@stanford.edu if you need access to STL files or other filetypes.</p>"},{"location":"collecting_data/","title":"Collecting Data","text":"<p>To collect data using the Trunk robot, after setting up the robot using the motion capture and motor control instructions, the following steps can be followed.</p>"},{"location":"collecting_data/#usage","title":"Usage","text":"<p>Essentially, all you need to run is contained in: <pre><code>cd asl_trunk_ws\n./scripts/run_data_collection.sh\n</code></pre> Currently, this script will only collect steady-state data according to the control inputs as specified in control_inputs.csv in the <code>asl_trunk_ws</code>. The data will be saved in the <code>data/steady_state/</code> directory.</p>"},{"location":"contributing/","title":"Contributing to the ASL Trunk robot project","text":"<p>Contributions are welcome! Here are some guidelines to follow when contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting started","text":"<p>Start by cloning this repository using the following command: <pre><code>gh repo clone hbuurmei/asl_trunk\n</code></pre> where the GitHub CLI is required to use the <code>gh</code> command (I highly recommend it).</p>"},{"location":"contributing/#project-layout","title":"Project layout","text":"<p>The project is organized as follows:</p> <pre><code>asl_trunk/\n    README.md  # The project README file.\n    asl_trunk/  # The main package.\n        asl_trunk_ws/  # The main ROS2 workspace, incl. data collection etc.\n        mocap_ws/  # The ROS2 workspace for interfacing with the motion capture system.\n        motor_control_ws/  # The ROS2 workspace for controlling the motors.\n    docs/\n        mkdocs.yml    # The website configuration file.\n        docs/\n            index.md  # The documentation homepage.\n            contributing.md  # This file.\n            ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"contributing/#code-contributions","title":"Code contributions","text":"<p>All the ROS2 packages are located in the <code>asl_trunk/</code> directory, and each workspace is their own repository. These are added via git subtrees to have everything in one place. Therefore, just contribute to the respective workspace repository, which will most likely be the asl_trunk_ws repository. Afterwards, the main repository can be updated with the new changes using the following command: <pre><code>git subtree pull --prefix=asl_trunk/asl_trunk_ws https://github.com/hbuurmei/asl_trunk_ws.git main\n</code></pre></p>"},{"location":"contributing/#contributing-to-the-documentation","title":"Contributing to the documentation","text":"<p>After cloning this repository, one can make updates to the documentation by editing the files in the <code>docs/</code> directory. The documentation is built using MkDocs, a static site generator that's geared towards project documentation. Specifically, we use the Material for MkDocs theme. This should be installed using the following command: <pre><code>pip install mkdocs-material\n</code></pre> Note: The documentation is built automatically using GitHub Actions, so there is no need to build it locally. Always push to the <code>main</code> branch. In case you want to preview the updates locally, simply use: <pre><code>mkdocs serve\n</code></pre> in the main directory, and open the browser as instructed.</p>"},{"location":"electrical_design/","title":"Electrical Design","text":""},{"location":"electrical_design/#system-description","title":"System description","text":"<p>The trunk is actuated by 6 CIM 12V motors, each with a Talon SRX controller and an encoder. The CIM motors are powered by a 12V, 100A power supply. A 20A circuit breaker is in series with the positive terminal of each motor controller to protect from current spikes. Low level motor commands are handled with a Raspberry Pi 4, which has its own 5V power supply. CAN is the protocol used to communicate commands from the Raspberry Pi to the motor controllers, via a CANable 1.0 device. The gripper servo has its own 6V power supply. The grounds of all power supplies are connected to a common ground, which is connected to the frame. </p>"},{"location":"electrical_design/#circuit-diagram","title":"Circuit diagram","text":""},{"location":"mechanical_design/","title":"Mechanical Design","text":"<p>The ASL Trunk robot is a low cost, highly customizable, open-source desktop soft robot hardware platform. The trunk is powered by 6 motors, which control 12 tendons that terminate at 3 disks along the length of the robot. Custom pretensioning mechanisms keep the antagonistc tendons in tension, and the actuation unit routes the tendons into the trunk. </p>"},{"location":"mechanical_design/#full-bom","title":"Full BOM","text":"<p>The full working bill of materials is available here . </p>"},{"location":"mechanical_design/#full-cad","title":"Full CAD","text":""},{"location":"mechanical_design/#trunk","title":"Trunk","text":"<p>The flexible body of the trunk is a standard vacuum hose, which was cut to length for our application. 3 custom 3d printed disks, which each have 12 radially symmetric channels, divide the trunk into 3 equal-length segments. At each disk, 4 tendons terminate. Each disk also has a unique arrangment of motion capture markers, so OptiTrack Motive can easily distinguish them from each other for pose estimation. A custom parallel jaw gripper, adapted from this design, is mounted on the end effector, driven by a small servo housed within the trunk body. The jaws of the gripper are easily swappable for different applications, including carrying large amounts of weight (up to 600g).</p>"},{"location":"mechanical_design/#actuation-unit","title":"Actuation Unit","text":"<p>The actuation unit routes 12 tendons from their respective pretensioning mechanisms to the trunk. The main structure is a custom 3d printed mount, which connects to the frame. 6 entry holes with 6 corresponding shafts hold 12 small pulleys which route the tendons with minimal friction and no overlap. The bottom of the actuation unit has a snap-fit attachment for the top of the trunk.</p>"},{"location":"mechanical_design/#pretensioning-mechanism-ptm","title":"Pretensioning Mechanism (PTM)","text":"<p>The pretensioning mechanism is heavily inspired by Yeshmukhametov et al., 2019. A pretensioning mechanism is necessary to drive two antagonistic cables with the same motor, such that when one is pulled by the motor, the other does not go slack. Our design consists of a \"sled\" that passively tensions a tendon using two compression springs in series on the lower linear rail.</p>"},{"location":"mechanical_design/#motor-assemblies","title":"Motor Assemblies","text":"<p>Each motor assembly is centered around a CIM 12V motor. We use the CIM12V mount along with a custom 3d printed mount to securely attach the motor and Talon SRX controller to the frame. A custom 3d printed pulley is connected to the motor shaft using a shaft key, and the tendons are secured to the top of the pulley. The Talon encoder is mounted to the frame using a custom 3d printed mount.</p>"},{"location":"mocap/","title":"Motion Capture System","text":""},{"location":"mocap/#usage","title":"Usage","text":"<p>First, make sure the robot is turned on. The motion capture cameras should show numbers 1-4. The Windows laptop has to be connected to the OptiHub via USB, and be running the Motive 2 software. Then, on the remote computer run the following command: <pre><code>cd mocap_ws\nsource install/setup.bash\nros2 launch mocap4r2_optitrack_driver optitrack2.launch.py\n</code></pre> and in a new terminal run: <pre><code>cd mocap_ws\nsource install/setup.bash\nros2 lifecycle set /mocap4r2_optitrack_driver_node activate\nros2 run converter converter_node\n</code></pre> You can choose whether to use markers or rigid bodies by changing the <code>type</code> parameter, i.e. <pre><code>ros2 run converter converter_node --ros-args -p type:='markers'  # or 'rigid_bodies' (default)\n</code></pre></p>"},{"location":"mocap/#troubleshooting","title":"Troubleshooting","text":"<p>In the Motive 2 software, make sure that the rigid bodies are correctly set up. There should be a rigid body for each segment of the robot, and the markers should be correctly assigned to the rigid bodies (5/6 markers per rigid body).</p>"},{"location":"motor_control/","title":"Motor Control","text":"<p>The motor controllers are connected to the Raspberry Pi 4 (4GB RAM), which has Ubuntu 20.04 installed. The motor controllers are controlled using the ROS2 Foxy framework (note not Humble distro!), which is already installed on the Pi.</p>"},{"location":"motor_control/#usage","title":"Usage","text":"<p>In the first terminal, run: <pre><code>cd Phoenix-Linux-SocketCAN-Example\nsudo ./canableStart.sh  # start the CANable interface\ncd ../motor_control_ws\nsource install/setup.bash\nros2 launch ros_phoenix trunk.launch.py\n</code></pre> In the second terminal, run: <pre><code>cd motor_control_ws\nsource install/setup.bash\nros2 run converter converter_node  # optionally add --ros-args -p debug:=true\n</code></pre></p>"},{"location":"motor_control/#motor-control-modes","title":"Motor control modes","text":"<p>The motor control modes are as follows:</p> Mode Value <code>PERCENT_OUTPUT</code> 0 <code>POSITION</code> 1 <code>VELOCITY</code> 2 <code>CURRENT</code> 3 <code>FOLLOWER</code> 5 <code>MOTION_PROFILE</code> 6 <code>MOTION_MAGIC</code> 7 <code>MOTION_PROFILE_ARC</code> 10 <p>To simply run a command once to test the motor control, the following command can be used:</p> <pre><code>ros2 topic pub --once /all_motors_control interfaces/msg/AllMotorsControl \"{motors_control: [{mode: 0, value: 0.25},{mode: 0, value: 0},{mode: 0, value: 0},{mode: 0, value: 0},{mode: 0, value: 0},{mode: 0, value: 0}]}\"\n</code></pre>"},{"location":"motor_control/#motor-control-limits","title":"Motor control limits","text":"<p>The motor control limits are empirically established as follows:</p> \\[ \\operatorname{norm}\\left(0.75\\left(\\vec{u}_3+\\vec{u}_4\\right)+1.0\\left(\\vec{u}_2+\\vec{u}_5\\right)+1.25\\left(\\vec{u}_1+\\vec{u}_6\\right)\\right) \\leq 0.6 \\] <p>Going beyond this limit can result in the robot going outside of the workspace, which can be dangerous.</p>"},{"location":"optitrack/","title":"OptiTrack System","text":""},{"location":"optitrack/#system-overview","title":"System Overview","text":"<p>What's needed to get the OptiTrack system up and running?</p>"},{"location":"optitrack/#camera-setup","title":"Camera Setup","text":""},{"location":"optitrack/#motive-20","title":"Motive 2.0","text":""},{"location":"optitrack/#calibration","title":"Calibration","text":""},{"location":"software_design/","title":"Software Design","text":""},{"location":"software_design/#ros-graph","title":"ROS Graph","text":""},{"location":"software_design/#teleoperation","title":"Teleoperation","text":""},{"location":"software_design/#overview","title":"Overview","text":"<p>The trunk robot is teleoperated by a user wearing an Apple Vision Pro. We designed an augmented reality app written in Swift which initializes a virtual 3d, 3-link spherical pendulum overlayed on the real-world view of the user. Once the virtual trunk is initialized, the user can calibrate the position and orientation of the virtual trunk to the hardware system. After calibration, the user can look at one of the disks on the trunk, which then lights up to denote its selection. The user can pinch their thumb and forefinger to select the disk, then the position of the virtual disk will mirror the position of their hand. The virtual disk positions can optionally be streamed over WiFi to a ROS listener, which publishes the 3d positions of the 3 disks on the trunk to the desired positions topic. A controller node subscribes to this topic and calculates the motor outputs necessary to attain that pose. The updated motor outputs are published to the motors, which causes the hardware trunk to mirror the virtual trunk. Streaming of desired trunk positions is done at 10Hz, and all of the other ROS functions run at 100Hz.</p>"},{"location":"software_design/#swift-app-design","title":"Swift App Design","text":"<p>The Apple Vision Pro teleoperation app was written in Swift 5 using XCode Beta 16 for VisionOS 2.0 Beta. Beta versions of XCode and VisionOS were used since some functionality necessary for our app was only available in beta versions. </p> <p>Our 3D assets were programmatically generated with standard hierarchical RealityKit Entities. The entities are placed into a MeshResource.Skeleton, upon which a custom IKComponent is added. A corresponding IKSolver smoothly solves the inverse kinematics of the 3 spherical pendulum joints when the position of the end effector is commanded with a gesture. The disk selection gestures are created with DragGestures. The streaming functionality for our app was heavily inspired by VisionProTeleop, using GRPC to stream disk positions over WiFi. </p> <p>Source code for the app can be found in this GitHub repository. </p>"},{"location":"telemetry_viewer/","title":"Telemetry Viewer","text":"<p>For visualizing telemetry data, such as the motor output currents, motor control temperatures, and webcam stream, we use the free foxglove tool.</p>"},{"location":"telemetry_viewer/#usage","title":"Usage","text":"<p>You can run the Foxglove server/ROS2 node by running the following command in any ROS2 workspace: <pre><code>ros2 launch foxglove_bridge foxglove_bridge.launch.xml\n</code></pre> Note that Foxglove is installed for a particular ROS2 distribution, but you can install it for any distribution, see below. Then, just open the Foxglove web interface via their website and connect. You will be able to visualize almost any data type. Finally, particular settings, such as topics to listen to, are stored and can be found in the repo.</p>"},{"location":"telemetry_viewer/#installing-foxglove","title":"Installing Foxglove","text":"<p>The only thing to do to run Foxglove is to install the WebSocket server. This can be done by running the following command: <pre><code>sudo apt install ros-$ROS_DISTRO-foxglove-bridge\n</code></pre></p>"},{"location":"teleoperation/","title":"Teleoperation","text":"<p>To collect data for visuomotor policies using the Trunk robot with teleoperation, or to just teleoperate the robot for a demonstration, first set up the robot using the motor control, motion capture, and video streaming directions, then the following steps can be followed.</p>"},{"location":"teleoperation/#usage","title":"Usage","text":"<p>Initialize a control solver node: <pre><code>cd asl_trunk_ws\nsource install/setup.bash\nros2 run controller ik_solver_node\n</code></pre></p> <p>Start an image storing node: <pre><code>cd asl_trunk_ws\nsource install/setup.bash\nros2 run streamer image_storing_node\n</code></pre></p> <p>Start an executor node: <pre><code>cd asl_trunk_ws\nsource install/setup.bash\nros2 run executor run_experiment_node \n</code></pre></p> <p>Then initialize the AVP streaming node: <pre><code>cd asl_trunk_ws\nsource install/setup.bash\nros2 run streamer avp_streamer_node\n</code></pre></p> <p>You can then follow the prompts in the TrunkTeleop App on the AVP to calibrate the virtual trunk to the hardware trunk, then start streaming and recording data from trunk teleoperation. </p>"},{"location":"teleoperation/#teleop-example","title":"Teleop Example","text":"<p>See the video below for an example of teleoperation in action.</p>"},{"location":"video_streaming/","title":"Video Streaming","text":"<p>The Trunk robot is equipped with a camera that can be used to stream video data to a remote computer. This can be useful for teleoperation, data collection, or other applications. The video stream is published as a ROS2 topic, which can be subscribed to by other nodes in the ROS2 network.</p>"},{"location":"video_streaming/#usage","title":"Usage","text":"<p>To start the video stream, run the following command on the PI: <pre><code>cd cam_ws\nsource install/setup.bash\nros2 run v4l2_camera v4l2_camera_node --ros-args -p image_size:=[1920,1080]\n</code></pre> This will start the video stream and publish the video data on the <code>/image</code> topic, and compressed video data on the <code>/image/compressed</code> topic. To alter for instance the frame rate or resolution, simply add the arguments as: <pre><code>ros2 run v4l2_camera v4l2_camera_node --ros-args -p image_size:=[1920,1080] -p framerate:=15\n</code></pre></p> <p>To subscribe to the video stream, run the following command on the remote computer inside any ROS2 workspace: <pre><code>ros2 run rqt_image_view rqt_image_view\n</code></pre> and select the appropriate topic to view the video stream, e.g. <code>/image/theora</code>. This can also be viewed directly with all the other data, as described in the telemetry viewer page.</p>"},{"location":"video_streaming/#re-installing","title":"Re-installing","text":"<p>For instance, this tutorial can be followed to re-install the camera driver. However, note that we installed Ubuntu 20.04 on the Pi, not Raspberry Pi OS (previously Raspbian), such that the ROS2 installation is different (much simpler). Specifically, once ROS2 is installed, the following commands can be used to install the camera packages: <pre><code>mkdir -p Documents/cam_ws/src &amp;&amp; cd Documents/cam_ws/src\ngit clone --branch foxy https://gitlab.com/boldhearts/ros2_v4l2_camera.git\ngit clone --branch foxy https://github.com/ros-perception/vision_opencv.git\ngit clone --branch foxy https://github.com/ros-perception/image_common.git\ngit clone --branch foxy-devel https://github.com/ros-perception/image_transport_plugins.git\ncd ..\nrosdep install --from-paths src -r -y\ncolcon build\nsource install/setup.bash\n</code></pre> Then, the camera can be started as described above. You may need allow the camera to be accessed by the user, which can be done by adding the user to the <code>video</code> group, or by adding the following udev rule: <pre><code>sudo nano /etc/udev/rules.d/99-webcam.rules\nKERNEL==\"video[0-9]*\", MODE=\"0666\"  # add this to the file\nsudo udevadm control --reload-rules\nsudo udevadm trigger\n</code></pre></p>"},{"location":"visuomotor_rollout/","title":"Visuomotor Rollout","text":"<p>To test a visuomotor policy using the Trunk robot hardware, first set up the robot using and motor control and video streaming instructions, then the following steps can be followed.</p>"},{"location":"visuomotor_rollout/#usage","title":"Usage","text":"<p>Initialize a control solver node: <pre><code>cd asl_trunk_ws\nsource install/setup.bash\nros2 run controller ik_solver_node\n</code></pre></p> <p>To start the visuomotor rollout, run these commands in a new terminal: <pre><code>cd asl_trunk_ws\nsource install/setup.bash\nros2 run executor visuomotor_node\n</code></pre></p> <p>The robot hardware will then react to vision input.</p>"},{"location":"visuomotor_rollout/#example-rollout","title":"Example Rollout","text":"<p>In the following video, a ResNet18 was trained to output desired trunk pose from an image of the robot enclosure. A set of ~90 training images were collected with AVP teleoperation to have the robot point toward the red octagon. Here's the rollout seen at 8x speed:</p>"},{"location":"visuomotor_rollout/#visuomotor-policy-code","title":"Visuomotor Policy Code","text":"<p>All code for training and testing visuomotor policies is available in this Github repo.</p>"}]}